\subsection*{Modeling}
\label{sec:results_modeling}
To quantitatively account for the generalization observed in each of the five experiments, we considered three potential means for computing the perceptual similarity of mixtures, and then fit a model for each.  
We considered three possible candidate models which differed in how and which components are involved in the computation of similarity (Figure \ref{fig:cartoon}).  
Each model contained 3 parameters: the baseline response probability ($p_0$), and the salience of individual components according to vapor pressure ($\alpha$), the steepness of generalization for components differing in carbon chain length ($\beta$).  (Figure \ref{fig:cartoon}) illustrates the intuitions [better word] of the three different models, where the specific modeling details and code may be found in the supplemental material and at [website with ipython notebook]. 

The computational complexity varied across models (Figure \ref{fig:cartoon}).  It was simplest in the ``\textit{Mean}'' model, consisting only of a single comparison of average carbon chain length of mixtures.  
It was most complex in the ``\textit{All-to-All}'' model, consisting of $N^2$ comparison for $N$-component mixtures ($N=2$ here).  The ``\textit{Nearest-Neighbor}'' model was intermediate in complexity because it consisted of $N$ comparisons.  